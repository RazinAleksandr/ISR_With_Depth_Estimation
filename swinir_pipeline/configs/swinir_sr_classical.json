{
  "task": "swinir_sr_classical_patch48_x2"     //  classical image sr for x2/x3/x4/x8. root/task/images-models-options
  , "model": "plain" // "plain" | "plain2" if two inputs
  , "gpu_ids": null //[0,1,2,3,4,5,6,7]
  , "dist": true

  , "scale": 2       // 2 | 3 | 4 | 8
  , "n_channels": 3  // broadcast to "datasets", 1 for grayscale, 3 for color
  , "n_channels_depth": 1  // broadcast to "datasets", 1 for grayscale, 3 for color

  , "path": {
    "root": "superresolution"            // "denoising" | "superresolution" | "dejpeg"
    , "pretrained_netG": "/home/arazin/main/work/HUAWEI/ISR/ISR_With_Depth_Estimation/SwinIR_with_depth/model_zoo/001_classicalSR_DIV2K_s48w8_SwinIR-M_x2.pth"      // path of pretrained model. We fine-tune X3/X4/X8 models from X2 model, so that `G_optimizer_lr` and `G_scheduler_milestones` can be halved to save time.
    , "pretrained_netE": null // "model_zoo/2200_E.pth"      // path of pretrained model
  }

  , "datasets": {
    "train": {
      "name": "train_dataset"           // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "/home/arazin/main/work/HUAWEI/ISR/ISR_With_Depth_Estimation/ldm_custom_pipeline/datasets/images/trainset"// path of H training dataset. DIV2K (800 training images)
      , "dataroot_L": null              // path of L training dataset

      , "H_size": 96                   // 96/144|192/384 | 128/192/256/512. LR patch size is set to 48 or 64 when compared with RCAN or RRDB.

      , "dataloader_shuffle": false
      , "dataloader_num_workers": 2
      , "dataloader_batch_size": 48      // batch size 1 | 16 | 32 | 48 | 64 | 128. Total batch size =4x8=32 in SwinIR
    }
    , "test": {
      "name": "test_dataset"            // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "/home/arazin/main/work/HUAWEI/ISR/ISR_With_Depth_Estimation/ldm_custom_pipeline/datasets/images/testset"  // path of H testing dataset
      , "dataroot_L": null              // path of L testing dataset
      , "H_size": 96
    }
    , "inference": {
      "name": "inference_dataset"            // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "datasets/inferencesets/HR"  // path of H inference dataset
      , "dataroot_L": null              // path of L inference dataset

    }
    
  }

  , "Depth_datasets": {
    "train": {
      "name": "depth_train_dataset"           // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "/home/arazin/main/work/HUAWEI/ISR/ISR_With_Depth_Estimation/ldm_custom_pipeline/datasets/depth_maps/trainset"// path of H training dataset. DIV2K (800 training images)
      , "dataroot_L": null              // path of L training dataset

      , "H_size": 96                   // 96/144|192/384 | 128/192/256/512. LR patch size is set to 48 or 64 when compared with RCAN or RRDB.

      , "dataloader_shuffle": false
      , "dataloader_num_workers": 2
      , "dataloader_batch_size": 48      // batch size 1 | 16 | 32 | 48 | 64 | 128. Total batch size =4x8=32 in SwinIR
    }
    , "test": {
      "name": "depth_test_dataset"            // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "/home/arazin/main/work/HUAWEI/ISR/ISR_With_Depth_Estimation/ldm_custom_pipeline/datasets/depth_maps/testset"  // path of H testing dataset
      , "dataroot_L": null              // path of L testing dataset

    }
    , "inference": {
      "name": "depth_inference_dataset"            // just name
      , "dataset_type": "sr"         // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "datasets/depth_estimation_datasets/inferencesets/HR"  // path of H inference dataset
      , "dataroot_L": null              // path of L inference dataset
      , "H_size": 96
    }
    
  }

  , "netG": {
    "net_type": "swinir" 
    , "upscale": 2                      // 2 | 3  | 4 | 8
    , "in_chans": 3 
    , "img_size": 48                    // For fair comparison, LR patch size is set to 48 or 64 when compared with RCAN or RRDB.
    , "window_size": 8  
    , "img_range": 1.0 
    , "depths": [6, 6, 6, 6, 6, 6] 
    , "embed_dim": 180 
    , "num_heads": [6, 6, 6, 6, 6, 6]
    , "mlp_ratio": 2 
    , "upsampler": "pixelshuffle"        // "pixelshuffle" | "pixelshuffledirect" | "nearest+conv" | null
    , "resi_connection": "3conv"        // "1conv" | "3conv"

    , "init_type": "default"
  }

  , "train": {
     "manual_seed": 17
    ,"G_lossfn_type": "l1"               // "l1" preferred | "l2sum" | "l2" | "ssim" | "charbonnier"
    , "G_lossfn_weight": 1.0            // default

    , "E_decay": 0.999                  // Exponential Moving Average for netG: set 0 to disable; default setting 0.999

    , "G_optimizer_type": "adam"        // fixed, adam is enough
    , "G_optimizer_lr": 1e-5            // learning rate
    , "G_optimizer_wd": 0               // weight decay, default 0
    , "G_optimizer_clipgrad": null      // unused
    , "G_optimizer_reuse": true         // 

    , "G_scheduler_type": "MultiStepLR" // "MultiStepLR" is enough
    , "G_scheduler_milestones": [1600, 1620, 1800]
    , "G_scheduler_gamma": 0.5

    , "G_regularizer_orthstep": null    // unused
    , "G_regularizer_clipstep": null    // unused

    , "G_param_strict": true
    , "E_param_strict": true

    , "checkpoint_test": 200           // for testing
    , "checkpoint_save": 200          // for saving model
    , "checkpoint_print": 20           // for print
  }
}
