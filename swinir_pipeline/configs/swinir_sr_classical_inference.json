{
  "task": "swinir_sr_classical_fine_tuned"               //"swinir_sr_classical_patch48_x2"     //  classical image sr for x2/x3/x4/x8. root/task/images-models-options
  , "model": "plain"                                                                      // "plain" | "plain2" if two inputs
  , "gpu_ids": [0]                                                                        //[0,1,2,3,4,5,6,7]
  , "dist": true

  , "scale": 2                                                                            // 2 | 3 | 4 | 8
  , "n_channels": 3                                                                       // broadcast to "datasets", 1 for grayscale, 3 for color
  , "n_channels_depth": 1                                                                 // broadcast to "datasets", 1 for grayscale, 3 for color

  , "path": {
    "root": "/home/aleks/main/huawei/swin_logdir"                                         // "denoising" | "superresolution" | "dejpeg"
    , "pretrained_netG": "/home/aleks/main/huawei/ISR_With_Depth_Estimation/pretrained_models/swin/790_G_.pth"      // path of pretrained model. We fine-tune X3/X4/X8 models from X2 model, so that `G_optimizer_lr` and `G_scheduler_milestones` can be halved to save time.
    , "pretrained_netE": "/home/aleks/main/huawei/ISR_With_Depth_Estimation/pretrained_models/swin/790_E_.pth"                                                             // "model_zoo/2200_E.pth" path of pretrained model
  }

  , "datasets": {
    "inference": {
      "name": "inference_dataset"                                                         // just name
      , "dataset_type": "sr"                                                              // "dncnn" | "dnpatch" | "fdncnn" | "ffdnet" | "sr" | "srmd" | "dpsr" | "plain" | "plainpatch" | "jpeg"
      , "dataroot_H": "/home/aleks/main/huawei/datasets/images/inferenceset"                   // path of H inference dataset
      , "depth_dataroot_H": "/home/aleks/main/huawei/datasets/depth_maps/inferenceset"  
      , "dataroot_L": null                                                                // path of L inference dataset
      , "depth_dataroot_L": null   
      , "H_size": 96
    
    }

  }

  , "netG": {
    "net_type": "swinir"
    , "upscale": 2                                                                        // 2 | 3  | 4 | 8
    , "in_chans": 3
    , "img_size": 48                                                                      // For fair comparison, LR patch size is set to 48 or 64 when compared with RCAN or RRDB.
    , "window_size": 8
    , "img_range": 1.0
    , "depths": [6, 6, 6, 6, 6, 6]
    , "embed_dim": 180
    , "num_heads": [6, 6, 6, 6, 6, 6]
    , "mlp_ratio": 2
    , "upsampler": "pixelshuffle"                                                         // "pixelshuffle" | "pixelshuffledirect" | "nearest+conv" | null
    , "resi_connection": "3conv"                                                          // "1conv" | "3conv"

    , "init_type": "default"
  }

, "train": {
  "epochs": 30
, "manual_seed": 17
, "G_lossfn_type": "l1"                                                               // "l1" preferred | "l2sum" | "l2" | "ssim" | "charbonnier"
, "G_lossfn_weight": 1.0                                                              // default

, "E_decay": 0.999                                                                    // Exponential Moving Average for netG: set 0 to disable; default setting 0.999

, "G_optimizer_type": "adam"                                                          // fixed, adam is enough
, "G_optimizer_lr": 1e-4                                                              // learning rate
, "G_optimizer_wd": 0                                                                 // weight decay, default 0
, "G_optimizer_clipgrad": null                                                        // unused
, "G_optimizer_reuse": true                                                           //

, "G_scheduler_type": "MultiStepLR"                                                   // "MultiStepLR" is enough
, "G_scheduler_milestones": [3000, 6000, 8000, 10000]
, "G_scheduler_gamma": 0.5

, "G_regularizer_orthstep": null                                                      // unused
, "G_regularizer_clipstep": null                                                      // unused

, "G_param_strict": true
, "E_param_strict": true

, "checkpoint_test": 636                                                              // for testing
, "checkpoint_save": 1273                                                             // for saving model
, "checkpoint_print": 200                                                             // for print
}
}